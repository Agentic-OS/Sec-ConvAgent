services:
  app:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./:/app:rw
      - vector_db:/app/vector_db
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - VECTOR_DB_PATH=${VECTOR_DB_PATH}
      - TEMPERATURE=${TEMPERATURE}
      - OLLAMA_API_KEY=${OLLAMA_API_KEY}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
    depends_on:
      - ollama
    networks:
      - ollama-docker
    container_name: streamlit-app
    restart: always

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    command: sh -c "ollama serve && ollama pull deepseek-r1:1.5b"
    networks:
      - ollama-docker

networks:
  ollama-docker:
    driver: bridge

volumes:
  vector_db:
    driver: local